{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Backtest  Train Size  Test Start        R2        MAE       RMSE  \\\n",
      "0          1         221  2023-01-01 -1.433869  11.504957  12.209856   \n",
      "1          2         241  2023-02-01 -1.970673   9.194563  11.410468   \n",
      "2          3         260  2023-03-01  0.597236   3.102336   3.733481   \n",
      "3          4         283  2023-04-01 -1.717406   4.003870   4.915230   \n",
      "4          5         302  2023-05-01 -0.851299   4.590791   5.319414   \n",
      "5          6         324  2023-06-01 -3.291608  10.810328  11.269171   \n",
      "6          7         345  2023-07-01  0.044578   5.366106   5.966788   \n",
      "7          8         365  2023-08-01  0.225459   4.227102   4.772658   \n",
      "8          9         388  2023-09-01  0.826256   2.821532   3.613066   \n",
      "9         10         408  2023-10-01 -0.558184   7.175192   9.513455   \n",
      "10        11         430  2023-11-01  0.654247   5.339988   5.814932   \n",
      "11        12         451  2023-12-01  0.471318   4.545192   5.428208   \n",
      "12        13         471  2024-01-01 -1.724036  10.772284  11.954691   \n",
      "13        14         492  2024-02-01 -0.998488   6.703443   7.839398   \n",
      "14        15         512  2024-03-01 -1.611973   6.707110   7.580096   \n",
      "15        16         532  2024-04-01  0.416046   4.988261   6.059135   \n",
      "16        17         554  2024-05-01  0.015860   7.821587   8.202541   \n",
      "17        18         576  2024-06-01 -7.139150  15.389107  17.645152   \n",
      "18        19         595  2024-07-01 -1.133650   8.332092  10.180391   \n",
      "19        20         617  2024-08-01  0.775282   6.141428   7.159419   \n",
      "20        21         639  2024-09-01  0.555532   5.956930   6.615409   \n",
      "21        22         659  2024-10-01 -0.863813   6.908772   7.671598   \n",
      "22        23         682  2024-11-01 -3.934288  18.756384  20.181428   \n",
      "23        24         702  2024-12-01  0.244752   5.780537   6.640897   \n",
      "24        25         723  2025-01-01 -0.078448   8.166740   9.411495   \n",
      "25        26         743  2025-02-01 -0.575955   7.376283   8.888599   \n",
      "26        27         762  2025-03-01 -5.982501  24.559118  27.201448   \n",
      "\n",
      "        Order Seasonal Order          AIC  \n",
      "0   (1, 1, 1)  (0, 0, 1, 12)  1264.151686  \n",
      "1   (1, 1, 1)  (0, 0, 1, 12)  1368.233214  \n",
      "2   (1, 1, 1)  (0, 0, 1, 12)  1489.491478  \n",
      "3   (1, 1, 1)  (0, 0, 1, 12)  1619.527839  \n",
      "4   (1, 1, 1)  (0, 0, 1, 12)  1720.155579  \n",
      "5   (1, 1, 1)  (0, 0, 1, 12)  1833.243820  \n",
      "6   (1, 1, 1)  (0, 0, 1, 12)  1948.793009  \n",
      "7   (1, 1, 1)  (0, 0, 1, 12)  2050.244513  \n",
      "8   (1, 1, 1)  (0, 0, 1, 12)  2164.961890  \n",
      "9   (1, 1, 1)  (0, 0, 1, 12)  2269.959381  \n",
      "10  (1, 1, 1)  (0, 0, 1, 12)  2386.085613  \n",
      "11  (1, 1, 1)  (0, 0, 1, 12)  2498.685544  \n",
      "12  (1, 1, 1)  (0, 0, 1, 12)  2611.118863  \n",
      "13  (1, 1, 1)  (0, 0, 1, 12)  2728.319324  \n",
      "14  (1, 1, 1)  (0, 0, 1, 12)  2840.216619  \n",
      "15  (1, 1, 1)  (0, 0, 1, 12)  2969.173804  \n",
      "16  (1, 1, 1)  (0, 0, 1, 12)  3082.176787  \n",
      "17  (1, 1, 1)  (0, 0, 1, 12)  3201.605743  \n",
      "18  (1, 1, 1)  (0, 0, 1, 12)  3301.253670  \n",
      "19  (1, 1, 1)  (0, 0, 1, 12)  3431.452618  \n",
      "20  (1, 1, 1)  (0, 0, 1, 12)  3548.553516  \n",
      "21  (1, 1, 1)  (0, 0, 1, 12)  3651.175603  \n",
      "22  (1, 1, 1)  (0, 0, 1, 12)  3781.389210  \n",
      "23  (1, 1, 1)  (0, 0, 1, 12)  3910.024624  \n",
      "24  (1, 1, 1)  (0, 0, 1, 12)  4045.532360  \n",
      "25  (1, 1, 1)  (0, 0, 1, 12)  4165.739650  \n",
      "26  (1, 1, 1)  (0, 0, 1, 12)  4280.713126  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load datasets\n",
    "data_path = \"data\"\n",
    "file_names = {\n",
    "    \"bitcoin\": \"bitcoin.csv\",\n",
    "    \"gold\": \"gold.csv\",\n",
    "    \"google_trends\": \"google_trends.csv\",\n",
    "    \"sp500\": \"sp500.csv\",\n",
    "    \"treasury_3m\": \"treasury_3m.csv\",\n",
    "    \"treasury_10y\": \"treasury_10y.csv\",\n",
    "    \"copper\": \"copper.csv\",\n",
    "    \"oil\": \"oil.csv\",\n",
    "    \"unemployment\": \"unemployment.csv\",\n",
    "    \"moex\": \"MOEX.csv\",\n",
    "    \"sse\": \"SSE.csv\",\n",
    "    \"stoxx\": \"STOXX_600.csv\"\n",
    "}\n",
    "\n",
    "data = {}\n",
    "for key, file in file_names.items():\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Standardize all column names\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "        # Rename date column to timestamp\n",
    "        if \"date\" in df.columns:\n",
    "            df.rename(columns={\"date\": \"timestamp\"}, inplace=True)\n",
    "\n",
    "        if \"timestamp\" not in df.columns:\n",
    "            continue  # Skip file if no usable timestamp column\n",
    "\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        df.replace({'.': np.nan}, inplace=True)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if col != \"timestamp\":\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        data[key] = df\n",
    "\n",
    "\n",
    "# Custom fix for unemployment column name\n",
    "if \"unemployment\" in data:\n",
    "    unemployment_df = data[\"unemployment\"]\n",
    "    unemployment_df.columns = unemployment_df.columns.str.strip().str.lower()\n",
    "    if \"unemployment\" in unemployment_df.columns:\n",
    "        unemployment_df.rename(columns={\"unemployment\": \"unemployment_rate\"}, inplace=True)\n",
    "    data[\"unemployment\"] = unemployment_df\n",
    "\n",
    "rename_map = {\n",
    "    \"bitcoin\": {\n",
    "        \"open\": \"bitcoin_open\", \"high\": \"bitcoin_high\", \"low\": \"bitcoin_low\",\n",
    "        \"close\": \"bitcoin_close\", \"volume\": \"bitcoin_volume\"\n",
    "    },\n",
    "    \"gold\": {\n",
    "        \"open\": \"gold_open\", \"high\": \"gold_high\", \"low\": \"gold_low\",\n",
    "        \"close\": \"gold_close\", \"volume\": \"gold_volume\"\n",
    "    },\n",
    "    \"oil\": {\n",
    "        \"open\": \"oil_open\", \"high\": \"oil_high\", \"low\": \"oil_low\",\n",
    "        \"close\": \"oil_close\", \"volume\": \"oil_volume\"\n",
    "    },\n",
    "    \"sp500\": {\n",
    "        \"close\": \"sp500_close\"  # already done, keep this\n",
    "    },\n",
    "    \"copper\": {\"price\": \"copper_price\"},\n",
    "    \"google_trends\": {\n",
    "        \"spx\": \"google_spx\", \"etf\": \"google_etf\",\n",
    "        \"index fund\": \"google_index_fund\", \"sp500\": \"google_sp500\"\n",
    "    },\n",
    "    \"unemployment\": {\"unemployment\": \"unemployment_rate\"},\n",
    "    \"treasury_3m\": {\"close\": \"treasury_3m\"},\n",
    "    \"treasury_10y\": {\"close\": \"treasury_10y\"},\n",
    "    \"moex\": {\"close\": \"moex_close\"},\n",
    "    \"sse\": {\"close\": \"sse_close\"},\n",
    "    \"stoxx\": {\"close\": \"stoxx_close\"}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for key, renames in rename_map.items():\n",
    "    if key in data:\n",
    "        data[key] = data[key].rename(columns=renames)\n",
    "\n",
    "# Merge and clean data\n",
    "sp500 = data[\"sp500\"]\n",
    "all_data = sp500[[\"timestamp\", \"sp500_close\"]]\n",
    "for key, df in data.items():\n",
    "    if key != \"sp500\":\n",
    "        all_data = all_data.merge(df, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "all_data.sort_values(\"timestamp\", inplace=True)\n",
    "all_data.fillna(method=\"ffill\", inplace=True)\n",
    "all_data.dropna(inplace=True)\n",
    "all_data = all_data.loc[:, all_data.nunique() > 1]\n",
    "\n",
    "# Feature Engineering\n",
    "all_data['sp500_lag_1'] = all_data['sp500_close'].shift(1)\n",
    "all_data['oil_lag_1'] = all_data['oil_close'].shift(1)\n",
    "all_data['unemp_lag_1'] = all_data['unemployment_rate'].shift(1)\n",
    "\n",
    "all_data['sp500_rolling_mean_7'] = all_data['sp500_close'].rolling(7).mean()\n",
    "all_data['sp500_rolling_std_30'] = all_data['sp500_close'].rolling(30).std()\n",
    "\n",
    "all_data['oil_x_unemployment'] = all_data['oil_close'] * all_data['unemployment_rate']\n",
    "\n",
    "all_data['month'] = all_data['timestamp'].dt.month\n",
    "all_data['sin_month'] = np.sin(2 * np.pi * all_data['month'] / 12)\n",
    "all_data['cos_month'] = np.cos(2 * np.pi * all_data['month'] / 12)\n",
    "\n",
    "all_data.dropna(inplace=True)\n",
    "\n",
    "# Define target and features\n",
    "target = \"sp500_close\"\n",
    "exclude_cols = [\"timestamp\", target]\n",
    "exog_vars = [col for col in all_data.columns if col not in exclude_cols]\n",
    "\n",
    "# Monthly SARIMAX backtest\n",
    "results = []\n",
    "start_date = pd.Timestamp(\"2023-01-01\")\n",
    "end_date = pd.Timestamp(\"2025-03-01\")\n",
    "current_date = start_date\n",
    "\n",
    "while current_date <= end_date:\n",
    "    next_month = current_date + pd.DateOffset(months=1)\n",
    "\n",
    "    train = all_data[all_data[\"timestamp\"] < current_date]\n",
    "    test = all_data[(all_data[\"timestamp\"] >= current_date) & (all_data[\"timestamp\"] < next_month)]\n",
    "\n",
    "    if len(train) < 100 or len(test) < 10:\n",
    "        current_date += pd.DateOffset(months=1)\n",
    "        continue\n",
    "\n",
    "    y_train = train[target]\n",
    "    y_test = test[target]\n",
    "    X_train = train[exog_vars]\n",
    "    X_test = test[exog_vars]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    best_aic = np.inf\n",
    "    best_model = None\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "\n",
    "    for p in [0, 1]:\n",
    "        for q in [0, 1]:\n",
    "            for P in [0, 1]:\n",
    "                for Q in [0, 1]:\n",
    "                    try:\n",
    "                        model = SARIMAX(\n",
    "                            y_train,\n",
    "                            exog=X_train_pca,\n",
    "                            order=(p, 1, q),\n",
    "                            seasonal_order=(P, 0, Q, 12),\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False\n",
    "                        )\n",
    "                        model_fit = model.fit(disp=False)\n",
    "                        if model_fit.aic < best_aic:\n",
    "                            best_aic = model_fit.aic\n",
    "                            best_model = model_fit\n",
    "                            best_order = (p, 1, q)\n",
    "                            best_seasonal_order = (P, 0, Q, 12)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "    if best_model:\n",
    "        forecast = best_model.forecast(steps=len(X_test_pca), exog=X_test_pca)\n",
    "        r2 = r2_score(y_test, forecast)\n",
    "        mae = mean_absolute_error(y_test, forecast)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "\n",
    "        results.append({\n",
    "            \"Backtest\": len(results) + 1,\n",
    "            \"Train Size\": len(train),\n",
    "            \"Test Start\": current_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"R2\": r2,\n",
    "            \"MAE\": mae,\n",
    "            \"RMSE\": rmse,\n",
    "            \"Order\": best_order,\n",
    "            \"Seasonal Order\": best_seasonal_order,\n",
    "            \"AIC\": best_aic\n",
    "        })\n",
    "\n",
    "    current_date += pd.DateOffset(months=1)\n",
    "\n",
    "# Save and print results\n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "results_df.to_csv(\"results/sarimax_backtest_with_features.csv\", index=False)\n",
    "print(results_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Load datasets\n",
    "data_path = \"data\"\n",
    "file_names = {\n",
    "    \"bitcoin\": \"bitcoin.csv\",\n",
    "    \"gold\": \"gold.csv\",\n",
    "    \"google_trends\": \"google_trends.csv\",\n",
    "    \"sp500\": \"sp500.csv\",\n",
    "    \"treasury_3m\": \"treasury_3m.csv\",\n",
    "    \"treasury_10y\": \"treasury_10y.csv\",\n",
    "    \"copper\": \"copper.csv\",\n",
    "    \"oil\": \"oil.csv\",\n",
    "    \"unemployment\": \"unemployment.csv\",\n",
    "    \"moex\": \"MOEX.csv\",\n",
    "    \"sse\": \"SSE.csv\",\n",
    "    \"stoxx\": \"STOXX_600.csv\",\n",
    "    \"vix\": \"vix.csv\",\n",
    "    \"spgsci\": \"spgsci.csv\"\n",
    "}\n",
    "\n",
    "data = {}\n",
    "for key, file in file_names.items():\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        logging.info(f\"Loading {file}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        if \"timestamp\" not in df.columns:\n",
    "            logging.warning(f\"{file} skipped: no 'timestamp' column\")\n",
    "            continue\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        df.replace({'.': np.nan}, inplace=True)\n",
    "        for col in df.columns:\n",
    "            if col != \"timestamp\":\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        data[key] = df\n",
    "    else:\n",
    "        logging.warning(f\"{file} not found\")\n",
    "\n",
    "# Rename columns\n",
    "rename_map = {\n",
    "    \"bitcoin\": {\"Close\": \"bitcoin_close\", \"Open\": \"bitcoin_open\", \"High\": \"bitcoin_high\", \"Low\": \"bitcoin_low\", \"Volume\": \"bitcoin_volume\"},\n",
    "    \"gold\": {\"Close\": \"gold_close\", \"Open\": \"gold_open\", \"High\": \"gold_high\", \"Low\": \"gold_low\", \"Volume\": \"gold_volume\"},\n",
    "    \"oil\": {\"Close\": \"oil_close\", \"Open\": \"oil_open\", \"High\": \"oil_high\", \"Low\": \"oil_low\", \"Volume\": \"oil_volume\"},\n",
    "    \"copper\": {\"price\": \"copper_price\"},\n",
    "    \"google_trends\": {\"SPX\": \"google_spx\", \"ETF\": \"google_etf\", \"index fund\": \"google_index_fund\", \"sp500\": \"google_sp500\"},\n",
    "    \"unemployment\": {\"Unemployment\": \"unemployment_rate\"},\n",
    "    \"treasury_3m\": {\"Close\": \"treasury_3m\"},\n",
    "    \"treasury_10y\": {\"Close\": \"treasury_10y\"},\n",
    "    \"sp500\": {\"Close\": \"sp500_close\"},\n",
    "    \"moex\": {\"Close\": \"moex_close\"},\n",
    "    \"sse\": {\"Close\": \"sse_close\"},\n",
    "    \"stoxx\": {\"Close\": \"stoxx_close\"},\n",
    "    \"vix\": {\"Close\": \"vix_close\"},\n",
    "    \"spgsci\": {\"Close\": \"spgsci_close\"}\n",
    "}\n",
    "\n",
    "for key, renames in rename_map.items():\n",
    "    if key in data:\n",
    "        data[key] = data[key].rename(columns=renames)\n",
    "\n",
    "# Merge datasets\n",
    "logging.info(\"Merging all dataframes\")\n",
    "sp500 = data[\"sp500\"]\n",
    "all_data = sp500[[\"timestamp\", \"sp500_close\"]]\n",
    "for key, df in data.items():\n",
    "    if key != \"sp500\":\n",
    "        all_data = all_data.merge(df, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "all_data.sort_values(\"timestamp\", inplace=True)\n",
    "all_data.fillna(method=\"ffill\", inplace=True)\n",
    "all_data.dropna(inplace=True)\n",
    "\n",
    "# Advanced Feature Engineering\n",
    "def add_advanced_features(df, base_cols, lags=[1, 3, 7], windows=[3, 7]):\n",
    "    for col in base_cols:\n",
    "        for lag in lags:\n",
    "            df[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
    "        \n",
    "        for window in windows:\n",
    "            roll_mean = df[col].rolling(window).mean()\n",
    "            roll_std = df[col].rolling(window).std()\n",
    "            roll_min = df[col].rolling(window).min()\n",
    "            roll_max = df[col].rolling(window).max()\n",
    "            roll_skew = df[col].rolling(window).skew()\n",
    "            roll_kurt = df[col].rolling(window).kurt()\n",
    "\n",
    "            df[f\"{col}_roll_mean_{window}\"] = roll_mean\n",
    "            df[f\"{col}_roll_std_{window}\"] = roll_std\n",
    "            df[f\"{col}_roll_min_{window}\"] = roll_min\n",
    "            df[f\"{col}_roll_max_{window}\"] = roll_max\n",
    "            df[f\"{col}_roll_skew_{window}\"] = roll_skew\n",
    "            df[f\"{col}_roll_kurt_{window}\"] = roll_kurt\n",
    "\n",
    "            # Z-score\n",
    "            df[f\"{col}_zscore_{window}\"] = (df[col] - roll_mean) / roll_std\n",
    "\n",
    "        # Percentage change\n",
    "        df[f\"{col}_pct_change_1\"] = df[col].pct_change(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "key_features = [\"sp500_close\", \"vix_close\", \"bitcoin_close\", \"oil_close\"]\n",
    "all_data = add_advanced_features(all_data, key_features)\n",
    "all_data.dropna(inplace=True)\n",
    "all_data = all_data.loc[:, all_data.nunique() > 1]\n",
    "logging.info(\"Final dataset shape with advanced features: %s\", all_data.shape)\n",
    "\n",
    "# Define target and features\n",
    "target = \"sp500_close\"\n",
    "exclude_cols = [\"timestamp\", target]\n",
    "exog_vars = [col for col in all_data.columns if col not in exclude_cols]\n",
    "\n",
    "# SARIMAX Backtesting\n",
    "results = []\n",
    "start_date = pd.Timestamp(\"2023-01-01\")\n",
    "end_date = pd.Timestamp(\"2025-03-01\")\n",
    "current_date = start_date\n",
    "\n",
    "while current_date <= end_date:\n",
    "    logging.info(f\"Backtest for month starting {current_date.strftime('%Y-%m-%d')}\")\n",
    "    next_month = current_date + pd.DateOffset(months=1)\n",
    "\n",
    "    train = all_data[all_data[\"timestamp\"] < current_date]\n",
    "    test = all_data[(all_data[\"timestamp\"] >= current_date) & (all_data[\"timestamp\"] < next_month)]\n",
    "\n",
    "    if len(train) < 100 or len(test) < 10:\n",
    "        logging.warning(\"Skipping month due to insufficient data\")\n",
    "        current_date += pd.DateOffset(months=1)\n",
    "        continue\n",
    "\n",
    "    y_train = train[target]\n",
    "    y_test = test[target]\n",
    "    X_train = train[exog_vars]\n",
    "    X_test = test[exog_vars]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    best_aic = np.inf\n",
    "    best_model = None\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "\n",
    "    for p in [0, 1]:\n",
    "        for q in [0, 1]:\n",
    "            for P in [0, 1]:\n",
    "                for Q in [0, 1]:\n",
    "                    try:\n",
    "                        model = SARIMAX(\n",
    "                            y_train,\n",
    "                            exog=X_train_pca,\n",
    "                            order=(p, 1, q),\n",
    "                            seasonal_order=(P, 0, Q, 12),\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False\n",
    "                        )\n",
    "                        model_fit = model.fit(disp=False)\n",
    "                        if model_fit.aic < best_aic:\n",
    "                            best_aic = model_fit.aic\n",
    "                            best_model = model_fit\n",
    "                            best_order = (p, 1, q)\n",
    "                            best_seasonal_order = (P, 0, Q, 12)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "    if best_model:\n",
    "        forecast = best_model.forecast(steps=len(X_test_pca), exog=X_test_pca)\n",
    "        r2 = r2_score(y_test, forecast)\n",
    "        mae = mean_absolute_error(y_test, forecast)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "\n",
    "        results.append({\n",
    "            \"Backtest\": len(results) + 1,\n",
    "            \"Train Size\": len(train),\n",
    "            \"Test Start\": current_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"R2\": r2,\n",
    "            \"MAE\": mae,\n",
    "            \"RMSE\": rmse,\n",
    "            \"Order\": best_order,\n",
    "            \"Seasonal Order\": best_seasonal_order,\n",
    "            \"AIC\": best_aic\n",
    "        })\n",
    "\n",
    "    current_date += pd.DateOffset(months=1)\n",
    "\n",
    "# Save Results\n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "results_df.to_csv(\"results/sarimax_backtest_advanced_features.csv\", index=False)\n",
    "logging.info(\"Saved results to results/sarimax_backtest_advanced_features.csv\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
